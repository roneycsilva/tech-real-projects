# ğŸ“¦ Fullstack Data API
> **Pipeline completo de engenharia de dados, cobrindo ETL, Data Quality, API REST e Deploy em Cloud.**

Este projeto apresenta, de forma prÃ¡tica e aplicada, todo o ciclo de vida de dados corporativos: desde a ingestÃ£o de dados brutos (**RAW**), passando pelo processamento, limpeza e padronizaÃ§Ã£o (**Data Quality / CURATED**), atÃ© a disponibilizaÃ§Ã£o desses dados atravÃ©s de uma **API REST escalÃ¡vel** em ambiente de nuvem (**Cloud**).  

O objetivo Ã© demonstrar como transformar informaÃ§Ãµes inconsistentes e nÃ£o estruturadas em **dados confiÃ¡veis, estruturados e prontos para consumo**, permitindo que aplicaÃ§Ãµes de negÃ³cio possam **acessar, analisar e utilizar os dados de forma segura e eficiente**.  

AlÃ©m disso, o projeto enfatiza boas prÃ¡ticas de engenharia de dados, incluindo:
- Tratamento e normalizaÃ§Ã£o de dados financeiros e de datas;  
- AutomaÃ§Ã£o de pipelines de ETL com Node.js;  
- Testes e validaÃ§Ã£o de endpoints via Postman;  
- Deploy automatizado em Cloud (Railway), com configuraÃ§Ã£o de variÃ¡veis de ambiente e CI/CD.

Com isso, o projeto nÃ£o apenas exemplifica **tÃ©cnicas de manipulaÃ§Ã£o de dados**, mas tambÃ©m reforÃ§a habilidades de **integraÃ§Ã£o entre desenvolvimento, dados e infraestrutura em nuvem**, essenciais para cenÃ¡rios reais de mercado.

---

## ğŸ¯ Objetivo do Projeto

Construir uma infraestrutura completa de dados ponta a ponta, simulando cenÃ¡rios reais do mercado, com foco em **qualidade, escalabilidade e automaÃ§Ã£o**.  

O projeto aborda as seguintes etapas crÃ­ticas:
- **IngestÃ£o:** Coleta e processamento de dados inconsistentes e nÃ£o padronizados, garantindo que todos os registros possam ser utilizados no pipeline.  
- **TransformaÃ§Ã£o (ETL):** Limpeza, padronizaÃ§Ã£o e normalizaÃ§Ã£o dos dados, incluindo tratamento de valores financeiros, datas e campos nulos, para criar dados confiÃ¡veis e estruturados.  
- **PersistÃªncia:** Armazenamento seguro e eficiente em banco de dados NoSQL (MongoDB Atlas), utilizando documentos e collections para suportar consultas complexas.  
- **Entrega:** ExposiÃ§Ã£o dos dados atravÃ©s de uma **API REST escalÃ¡vel**, permitindo que aplicaÃ§Ãµes e usuÃ¡rios acessem informaÃ§Ãµes tratadas de forma confiÃ¡vel e em tempo real.

---
## ğŸŸ¢ Etapas do Projeto

ğŸŸ¢ **Fase 1 â€“ MongoDB:** ConfiguraÃ§Ã£o do cluster, Collections, Connection String, firewall.  
ğŸŸ¡ **Fase 2 â€“ ETL e Limpeza:** Regex para remover sÃ­mbolos, conversÃ£o de tipos, CSV â†’ JSON.  
ğŸ”µ **Fase 3 â€“ ValidaÃ§Ã£o com Postman:** Teste de requests, Status Codes 201/200, dados confirmados no cluster.  
ğŸŸ£ **Fase 4 â€“ Deploy e Cloud:** GitHub â†’ Railway CI/CD, variÃ¡veis de ambiente, link pÃºblico para API.  

---

## ğŸ§± Estrutura do Projeto

```text
 ğŸ“¦ fullstack-data-api
â”£ ğŸ“‚ data
â”ƒ â”£ ğŸ“‚ raw
â”ƒ â”ƒ â”— vendas_raw.csv
â”ƒ â”£ ğŸ“‚ curated
â”ƒ â”ƒ â”— vendas_curated.csv
â”ƒ â”— ğŸ“„ README.md
â”£ ğŸ“‚ img
â”ƒ â”— (prints do MongoDB, Postman e Deploy)
â”£ ğŸ“‚ src
â”ƒ â”— etl.js
â”£ ğŸ“„ server.js
â”£ ğŸ“„ package.json
â”£ ğŸ“„ .gitignore
â”— ğŸ“„ README.md
```

## ğŸ“Š Camada de Dados e PersistÃªncia NoSQL

Esta seÃ§Ã£o detalha como os dados sÃ£o armazenados, tratados e disponibilizados para consumo via API, mostrando a evoluÃ§Ã£o do pipeline de ETL desde os arquivos brutos (**RAW**) atÃ© os dados confiÃ¡veis (**CURATED**) no MongoDB Atlas.

### ğŸ“¥ Download dos Dados

VocÃª pode baixar os arquivos gerados pelo pipeline para acompanhar a evoluÃ§Ã£o dos dados:

ğŸ”´ **Dados Brutos (RAW)**  
[â¬‡ï¸ Download vendas_raw.csv](raw/vendas_raw.csv)  
_Este arquivo contÃ©m os dados originais, com inconsistÃªncias, sÃ­mbolos monetÃ¡rios (`R$`), datas no padrÃ£o brasileiro e possÃ­veis erros de digitaÃ§Ã£o._

ğŸŸ¢ **Dados Tratados (CURATED)**  
[â¬‡ï¸ Download vendas_curated.json](curated/vendas_curated.json)  
_Este arquivo contÃ©m dados limpos, padronizados e tipados, prontos para anÃ¡lise e consumo via API._

---

### ğŸ”´ Dados RAW (Sujo)

Esses dados representam o **estado original** antes do processamento. SÃ£o Ãºteis para auditoria e estudo da evoluÃ§Ã£o do pipeline, mas **nÃ£o devem ser usados diretamente** em anÃ¡lises ou aplicaÃ§Ãµes.

**CaracterÃ­sticas dos dados RAW:**
- Valores monetÃ¡rios inconsistentes (`R$`, `.`, `-`)  
- Datas em mÃºltiplos formatos  
- Campos vazios e nulos  
- Status nÃ£o padronizados (`Pago`, `pago`, `PENDENTE`, etc.)  
- Erros de digitaÃ§Ã£o (`canceldo`, `entregueu`, etc.)  

---

### ğŸŸ¢ Dados CURATED (Tratado)

ApÃ³s o processo de ETL, os dados foram transformados em **documentos consistentes e confiÃ¡veis**, ideais para persistÃªncia no MongoDB e consumo via API.

**TransformaÃ§Ãµes aplicadas:**
- ConversÃ£o de valores monetÃ¡rios para **Number**  
- NormalizaÃ§Ã£o de datas no padrÃ£o ISO 8601 (`YYYY-MM-DD`)  
- PadronizaÃ§Ã£o de status (`PAGO`, `PENDENTE`, `ENTREGUE`, `CANCELADO`)  
- RemoÃ§Ã£o de registros invÃ¡lidos ou incompletos  
- Estrutura consistente para fÃ¡cil consulta e anÃ¡lise  

Esses dados podem ser utilizados diretamente em **anÃ¡lises, dashboards ou aplicaÃ§Ãµes** sem risco de inconsistÃªncias.

---

## ğŸ”„ Pipeline ETL

O pipeline ETL (Extract, Transform, Load) Ã© responsÃ¡vel por **transformar os dados brutos (RAW) em dados confiÃ¡veis e estruturados (CURATED)**, garantindo que estejam prontos para anÃ¡lise ou consumo via API.

### 1ï¸âƒ£ Extract (ExtraÃ§Ã£o)
- Leitura dos arquivos CSV brutos (`raw`) utilizando Node.js.  
- Coleta e ingestÃ£o dos dados originais, mantendo registro completo para auditoria e rastreabilidade.  
- PreparaÃ§Ã£o dos dados para processamento subsequente, garantindo compatibilidade com o pipeline.

### 2ï¸âƒ£ Transform (TransformaÃ§Ã£o)
- **Limpeza de dados:** remoÃ§Ã£o de sÃ­mbolos monetÃ¡rios (`R$`, `.`, `-`) e caracteres especiais indesejados.  
- **ConversÃ£o de tipos:** transformaÃ§Ã£o de strings numÃ©ricas em valores do tipo `Number` e datas em formato ISO (`YYYY-MM-DD`).  
- **PadronizaÃ§Ã£o de texto:** unificaÃ§Ã£o de campos como status (`PAGO`, `PENDENTE`, `ENTREGUE`, `CANCELADO`).  
- **NormalizaÃ§Ã£o de datas:** ajuste de mÃºltiplos formatos encontrados nos dados brutos.  
- **Tratamento de valores ausentes:** preenchimento, substituiÃ§Ã£o ou exclusÃ£o de registros incompletos, garantindo consistÃªncia e integridade.

### 3ï¸âƒ£ Load (Carregamento)
- InserÃ§Ã£o dos dados tratados na **coleÃ§Ã£o MongoDB** (`CURATED`).  
- CriaÃ§Ã£o de arquivos JSON padronizados prontos para anÃ¡lise ou consumo via API.  
- Garantia de consistÃªncia e integridade, permitindo consultas confiÃ¡veis e rÃ¡pidas no banco de dados.

> Esse pipeline garante que dados originalmente inconsistentes se tornem **documentos estruturados e confiÃ¡veis**, permitindo anÃ¡lises precisas e integraÃ§Ã£o segura com sistemas externos.

---
## ğŸ–¥ï¸ Script Principal

O script principal (`etl.js`) Ã© o coraÃ§Ã£o do pipeline de dados. Ele automatiza **todo o fluxo de transformaÃ§Ã£o**, garantindo que os dados brutos (**RAW**) sejam convertidos em dados confiÃ¡veis (**CURATED**) e prontos para consumo via API.

Principais funÃ§Ãµes do script:

- **Leitura do CSV bruto:** captura todos os registros originais, mantendo rastreabilidade.  
- **Limpeza e transformaÃ§Ã£o de dados:**  
  - RemoÃ§Ã£o de sÃ­mbolos monetÃ¡rios (`R$`, `.`, `-`)  
  - ConversÃ£o de strings em nÃºmeros e datas em formato ISO (`YYYY-MM-DD`)  
  - PadronizaÃ§Ã£o de campos de status (`PAGO`, `PENDENTE`, `ENTREGUE`, `CANCELADO`)  
  - Tratamento de valores ausentes e registros invÃ¡lidos  
- **PersistÃªncia no MongoDB Atlas:** gravaÃ§Ã£o automÃ¡tica dos documentos tratados na coleÃ§Ã£o CURATED.  
- **GeraÃ§Ã£o de arquivo JSON:** mantÃ©m uma versÃ£o externa dos dados tratados para download e anÃ¡lise.

> Esse script garante que o pipeline seja **automÃ¡tico, auditÃ¡vel e escalÃ¡vel**, servindo como base para anÃ¡lises e integraÃ§Ã£o com aplicaÃ§Ãµes externas.

### âŒ InconsistÃªncia de Tipos
- **Problema:** Valores originalmente como strings impediam cÃ¡lculos, agregaÃ§Ãµes e consultas corretas no MongoDB.  
- **SoluÃ§Ã£o:** SanitizaÃ§Ã£o completa dos dados antes do `insertMany`, convertendo strings em **Number** e datas para formato ISO, garantindo consistÃªncia.

### âŒ Falha de ConexÃ£o no Deploy
- **Problema:** A API nÃ£o conseguia se conectar ao MongoDB devido a regras de firewall e restriÃ§Ãµes de rede.  
- **SoluÃ§Ã£o:** ConfiguraÃ§Ã£o de **IP Whitelist (`0.0.0.0/0`)** no MongoDB Atlas e uso de **variÃ¡veis de ambiente** para conexÃ£o segura.

### âŒ App Crash no Startup
- **Problema:** A aplicaÃ§Ã£o travava quando a porta padrÃ£o jÃ¡ estava em uso.  
- **SoluÃ§Ã£o:** ImplementaÃ§Ã£o de **porta dinÃ¢mica** utilizando `process.env.PORT`, permitindo que a aplicaÃ§Ã£o seja iniciada em qualquer ambiente de produÃ§Ã£o.
---

## â˜ï¸ Deploy & ValidaÃ§Ã£o

O projeto foi disponibilizado em ambiente de **produÃ§Ã£o escalÃ¡vel** utilizando Railway, garantindo que a API esteja sempre acessÃ­vel e pronta para consumo.

### 1ï¸âƒ£ Infraestrutura no Railway
- A API estÃ¡ conteinerizada e operando em ambiente de produÃ§Ã£o, com escalabilidade automÃ¡tica.  
- ConfiguraÃ§Ã£o de **variÃ¡veis de ambiente** para conexÃ£o segura ao MongoDB Atlas, evitando exposiÃ§Ã£o de senhas ou informaÃ§Ãµes sensÃ­veis.  

### 2ï¸âƒ£ ValidaÃ§Ã£o via Postman
- Teste de escrita (POST) simulando inserÃ§Ã£o de registros, por exemplo, para a IBM Brasil.  
- VerificaÃ§Ã£o de **Status Codes** (201 Created, 200 OK) para garantir que a integraÃ§Ã£o entre API e banco de dados esteja funcionando corretamente.  
- ConfirmaÃ§Ã£o de persistÃªncia dos dados no MongoDB Atlas.

**Resumo da validaÃ§Ã£o:**
- **Infraestrutura:** Railway (PaaS)  
- **Banco de Dados:** MongoDB Atlas (Cloud)  
- **Testes:** Postman, MongoDB Compass  
- **OperaÃ§Ãµes realizadas:** InserÃ§Ã£o de novos registros, leitura via endpoint pÃºblico e persistÃªncia confirmada no banco.

> Essa validaÃ§Ã£o garante que a API esteja funcional, segura e pronta para ser consumida por aplicaÃ§Ãµes externas ou usuÃ¡rios finais.

---

## ğŸš€ Endpoint da API

O projeto disponibiliza os dados tratados via **API REST**, permitindo que aplicaÃ§Ãµes externas ou usuÃ¡rios acessem os dados de forma confiÃ¡vel e em tempo real.

### ğŸ”¹ Endpoint PÃºblico

- **GET:** `https://api-vendas-roney-production.up.railway.app/meus-dados`  
  Retorna todos os registros da base **CURATED** em formato JSON, prontos para anÃ¡lise ou consumo por sistemas externos.

### ğŸ”¹ PossÃ­veis operaÃ§Ãµes adicionais (exemplos)

- **POST:** Inserir novos registros na base de dados (validaÃ§Ã£o de payload e status code 201 Created).  
- **PUT/PATCH:** Atualizar registros existentes (validaÃ§Ã£o de campos e consistÃªncia).  
- **DELETE:** Remover registros especÃ­ficos, mantendo integridade dos dados.

> Com este endpoint, qualquer aplicaÃ§Ã£o ou ferramenta de anÃ¡lise pode acessar os dados **tratados e confiÃ¡veis**, integrando facilmente com dashboards, scripts de anÃ¡lise ou sistemas de BI.

---

## ğŸ§° Stack TecnolÃ³gica

Este projeto utiliza tecnologias modernas para construir um pipeline completo de dados, do processamento Ã  entrega via API em nuvem:

- **Runtime:** Node.js â€“ ambiente de execuÃ§Ã£o para scripts de servidor.  
- **Framework:** Express.js â€“ criaÃ§Ã£o de APIs REST escalÃ¡veis e eficientes.  
- **Banco de Dados:** MongoDB Atlas â€“ banco NoSQL em nuvem, ideal para documentos e consultas flexÃ­veis.  
- **Deploy:** Railway â€“ PaaS para deploy automÃ¡tico e escalÃ¡vel da API.  
- **Testes e ValidaÃ§Ã£o:**  
  - **Postman:** teste de endpoints, validaÃ§Ã£o de payload e status codes.  
  - **MongoDB Compass:** inspeÃ§Ã£o e validaÃ§Ã£o de dados no cluster.  
  - **GitHub:** versionamento, integraÃ§Ã£o contÃ­nua (CI/CD) e controle de histÃ³rico do cÃ³digo.  

> Essa stack permite criar um pipeline **robusto, escalÃ¡vel e confiÃ¡vel**, integrando engenharia de dados, API REST e deploy em nuvem.

## ğŸ–¼ï¸ Prints e VisualizaÃ§Ãµes

A seguir, imagens que ilustram diferentes etapas do projeto, desde seguranÃ§a e configuraÃ§Ã£o da API atÃ© visualizaÃ§Ã£o e validaÃ§Ã£o dos dados no MongoDB Atlas.

<!-- SeguranÃ§a e ConfiguraÃ§Ã£o da API -->
<h4>ğŸ”¹ SeguranÃ§a e ConfiguraÃ§Ã£o da API</h4>
<table>
<tr>
<td><img src="img/01-seguranca.png" alt="SeguranÃ§a" width="300px"></td>
<td><img src="img/02-firewall.png" alt="Firewall" width="300px"></td>
<td><img src="img/03-variaveis-ambiente.png" alt="VariÃ¡veis de Ambiente" width="300px"></td>
</tr>
</table>

<!-- Testes com Postman -->
<h4>ğŸ”¹ Testes com Postman</h4>
<table>
<tr>
<td><img src="img/01-postman-criando-pedido.png" alt="Criando Pedido" width="300px"></td>
<td><img src="img/04-postman-get.png" alt="GET de Dados" width="300px"></td>
</tr>
</table>

<!-- Deploy e Monitoramento -->
<h4>ğŸ”¹ Deploy e Monitoramento</h4>
<table>
<tr>
<td><img src="img/02-railway-status-online.png" alt="Railway Status Online" width="300px"></td>
<td><img src="img/05-google-search.png" alt="Google Search" width="300px"></td>
</tr>
</table>

<!-- VisualizaÃ§Ã£o no MongoDB Atlas -->
<h4>ğŸ”¹ VisualizaÃ§Ã£o no MongoDB Atlas</h4>
<table>
<tr>
<td><img src="img/06-mongodb-compass.png" alt="MongoDB Compass" width="300px"></td>
<td><img src="img/07-mongodb-schema.png" alt="MongoDB Schema" width="300px"></td>
<td><img src="img/08-mongodb-documents.png" alt="MongoDB Documents" width="300px"></td>
</tr>
<tr>
<td><img src="img/09-mongodb-aggregations.png" alt="MongoDB Aggregations" width="300px"></td>
<td><img src="img/10-mongodb-indexes.png" alt="MongoDB Indexes" width="300px"></td>
</tr>
</table>


## ğŸ“Œ ObservaÃ§Ãµes Importantes

- Todos os dados utilizados neste projeto sÃ£o **fictÃ­cios** e servem **exclusivamente para fins educacionais**, garantindo que nenhuma informaÃ§Ã£o sensÃ­vel ou pessoal real seja armazenada ou processada.  
- O projeto foi desenvolvido com foco em **aprendizado e portfÃ³lio**, demonstrando habilidades prÃ¡ticas em diferentes Ã¡reas da engenharia de dados e desenvolvimento de software:  
  - **Engenharia de Dados (ETL e Data Quality):** coleta, transformaÃ§Ã£o, limpeza e normalizaÃ§Ã£o de dados, convertendo registros brutos em informaÃ§Ãµes confiÃ¡veis e estruturadas.  
  - **APIs RESTful:** criaÃ§Ã£o de endpoints escalÃ¡veis para consumo seguro e integrado com sistemas externos.  
  - **Deploy em Cloud (Railway):** disponibilizaÃ§Ã£o da API em ambiente de produÃ§Ã£o, com configuraÃ§Ã£o de variÃ¡veis de ambiente e prÃ¡ticas de CI/CD.  
- Todas as etapas seguem **boas prÃ¡ticas de desenvolvimento**, incluindo:  
  - Tratamento e padronizaÃ§Ã£o de dados inconsistentes  
  - Uso de **variÃ¡veis de ambiente** para proteÃ§Ã£o de credenciais  
  - Versionamento e integraÃ§Ã£o contÃ­nua (**CI/CD**) para rastreabilidade e atualizaÃ§Ãµes automÃ¡ticas  
- Este projeto Ã© ideal para estudo, demonstraÃ§Ã£o de competÃªncias tÃ©cnicas e referÃªncia em **projetos acadÃªmicos ou profissionais**, servindo como exemplo completo de pipeline de dados, API REST e deploy em nuvem.

---

## ğŸ‘¨â€ğŸ’» Autor

**Roney Cesar**  
Analista em Desenvolvimento de Sistemas, com foco na integraÃ§Ã£o entre **desenvolvimento de software e engenharia de dados**.  

Este projeto foi desenvolvido como estudo educacional, com Ãªnfase em:  
- **ETL (Extract, Transform, Load)** â€“ processamento e limpeza de dados brutos para geraÃ§Ã£o de informaÃ§Ãµes confiÃ¡veis.  
- **APIs RESTful** â€“ criaÃ§Ã£o de endpoints escalÃ¡veis para consumo seguro de dados.  
- **Deploy em Cloud (Railway)** â€“ publicaÃ§Ã£o de serviÃ§os em produÃ§Ã£o, com CI/CD e variÃ¡veis de ambiente.  

O objetivo Ã© demonstrar competÃªncias prÃ¡ticas e fornecer um **exemplo completo de pipeline de dados e API** para portfÃ³lio, aprendizado ou referÃªncia em projetos acadÃªmicos e profissionais.

---
